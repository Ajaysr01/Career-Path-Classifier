# -*- coding: utf-8 -*-
"""svm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jpClwkFPeCr_2MW5xKdp7uX9bxtMeryo
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix, roc_curve, f1_score, roc_auc_score
)

import warnings
warnings.filterwarnings("ignore")

#uploading the dataset
df = pd.read_csv("dataset9000.csv")

print('EXPLORATORY DATA ANALYSIS (EDA)')

# Basic Information
print(f'\nDATASET OVERVIEW')
print(f'Dataset Shape: {df.shape}')
print(f'Total Samples: {df.shape[0]:,}')
print(f'Total Features: {df.shape[1]-1} (excluding target)')
print(f'\nColumn Names:\n{list(df.columns)}')
print(f'\nData Types:\n{df.dtypes.value_counts()}')


print(f'\nMISSING VALUES ANALYSIS')
missing = df.isnull().sum()
if missing.sum() == 0:
    print('No missing values detected')
else:
    missing_df = pd.DataFrame({
        'Column': missing.index,
        'Missing_Count': missing.values,
    })
print(missing_df[missing_df['Missing_Count'] > 0])
print("Missing values fixed")


# Duplicate Analysis
print(f'\nDUPLICATE ANALYSIS')
duplicates = df.duplicated().sum()
print(f'Duplicate Rows: {duplicates}')
if duplicates > 0:
    print(f'{duplicates} duplicates detected. Removing them...')
    df = df.drop_duplicates()
    print(f'Dataset after removing duplicates: {df.shape}')
else:
    print('No duplicate rows found')

# Encoding Categorical Data
df_encoded = df.copy()
encoder_dict = {}
for col in df.columns:
    if df[col].dtype == 'object':
        # Fill NaN values with a placeholder string and ensure all values are strings
        df_encoded[col] = df_encoded[col].fillna('Missing').astype(str)
        le = LabelEncoder()
        df_encoded[col] = le.fit_transform(df_encoded[col])
        encoder_dict[col] = le

# Split Features & Labels
X = df_encoded.drop("Role", axis=1)
y = df_encoded["Role"]

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f'\n CORRELATION ANALYSIS')
print('\nTop 10 Features Most Correlated with Target:')
correlations = df_encoded.corr()['Role'].abs().sort_values(ascending=False)[1:11]
print(correlations.round(4))

# Correlation Heatmap
plt.figure(figsize=(14, 12))
correlation_matrix = df_encoded.corr()
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool), k=1)
sns.heatmap(correlation_matrix, mask=mask, annot=False, cmap='coolwarm',
            center=0, square=True, linewidths=0.5, cbar_kws={"shrink": 0.8})
plt.title('Feature Correlation Heatmap', fontsize=14, fontweight='bold', pad=20)
plt.tight_layout()
plt.show()

TARGET_C = 1.5
TARGET_GAMMA = 0.002

print(f"\nTraining Model with C={TARGET_C}, Gamma={TARGET_GAMMA}: ")

# SVM Pipeline
target_pipeline = Pipeline([
    ("scaler", StandardScaler()),
    ("svm", SVC(C=TARGET_C, gamma=TARGET_GAMMA, kernel='rbf', probability=True, random_state=42))
])

# Train the model
target_pipeline.fit(X_train, y_train)
best_model = target_pipeline

# Predictions
y_train_pred = best_model.predict(X_train)
y_test_pred = best_model.predict(X_test)
y_proba = best_model.predict_proba(X_test)

# Evaluate Performance
print(f"\nTRAINING PERFORMANCE (C={TARGET_C})")
print(f"Train Accuracy: {accuracy_score(y_train, y_train_pred):.4f}")

print(f"\nTEST PERFORMANCE (C={TARGET_C})")
test_accuracy = accuracy_score(y_test, y_test_pred)
print(f"Test Accuracy: {test_accuracy:.4f}")

print(classification_report(y_test, y_test_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title(f"Confusion Matrix - SVM (C={TARGET_C})")
plt.show()

# Cross-Validation Scoring
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_validate(best_model, X, y, cv=cv,
                         scoring=["accuracy", "f1_weighted"],
                         return_train_score=True)

print("\nCross Validation Results:")
print(f"Train Acc: {scores['train_accuracy'].mean():.4f}")
print(f"Test Acc:  {scores['test_accuracy'].mean():.4f}")

print(f"Train-Test Gap: {(scores['train_accuracy'].mean() - scores['test_accuracy'].mean()):.4f}\n")

# Calculate ROC-AUC Score for multiclass
test_roc_auc = roc_auc_score(y_test, y_proba, multi_class='ovr', average='weighted')


# ROC Curves for Each Class
if len(np.unique(y)) > 2:
    plt.figure(figsize=(10, 8))
    for cls in np.unique(y):
        y_true_bin = (y_test == cls).astype(int)
        fpr, tpr, _ = roc_curve(y_true_bin, y_proba[:, cls])
        plt.plot(fpr, tpr, label=f"Class {cls}")

    plt.plot([0, 1], [0, 1], "k--")
    plt.title(f"ROC Curves (One-vs-Rest) - C={TARGET_C}")
    plt.legend()
    plt.show()

print('FINAL MODEL SUMMARY')
print(f'''
Model Configuration:
  • Algorithm: Support Vector Classifier (SVC)
  • Kernel: RBF (Radial Basis Function)
  • C Parameter: 1.5
  • Gamma: 0.002
  • Max Iterations: 1000

Data Configuration:
  • Total Samples: {len(X):,}
  • Features: {X.shape[1]}
  • Target Classes: {'Role'}
  • Train/Test Split: 80/20
  • Cross-Validation Folds: 5
  • Feature Scaling: StandardScaler

Performance Summary:
  • Test Accuracy: {accuracy_score(y_test, y_test_pred):.4f}
  • Test F1-Score: {f1_score(y_test, y_test_pred, average='weighted'):.4f}
  • CV Mean Accuracy: {scores["test_accuracy"].mean():.4f} (±{scores["test_accuracy"].std():.4f})
  • ROC-AUC Score: {test_roc_auc:.4f}

Overfitting Analysis:
  • Train-Test Accuracy Gap: {accuracy_score(y_train, y_train_pred) - accuracy_score(y_test, y_test_pred):.4f}
  • Status: {"Excellent generalization" if accuracy_score(y_train, y_train_pred) - accuracy_score(y_test, y_test_pred) < 0.05 else "Good generalization" if accuracy_score(y_train, y_train_pred) - accuracy_score(y_test, y_test_pred) < 0.10 else "Potential overfitting"}

''')